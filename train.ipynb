{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1629dd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Metal)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import tqdm\n",
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "# from torch import nn, Tensor\n",
    "# from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from adam_atan2_pytorch import AdoptAtan2\n",
    "\n",
    "from titans_pytorch import (\n",
    "    MemoryAsContextTransformer,\n",
    "    MemoryMLP,\n",
    "    MemoryAttention\n",
    ")\n",
    "\n",
    "# device setup - auto-detect best available device\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('Using CUDA')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print('Using MPS (Metal)')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Using CPU (will be slow)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1178bb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "NUM_BATCHES = 2500\n",
    "BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATE_EVERY = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "VALIDATE_EVERY  = 100\n",
    "GENERATE_EVERY  = 250\n",
    "PRIME_LENGTH = 100\n",
    "GENERATE_LENGTH = 512\n",
    "SHOULD_GENERATE = True\n",
    "SEQ_LEN = 256\n",
    "\n",
    "# neural memory related\n",
    "\n",
    "NEURAL_MEMORY_DEPTH = 2\n",
    "NUM_PERSIST_MEM = 4\n",
    "NUM_LONGTERM_MEM = 4\n",
    "NEURAL_MEM_LAYERS = (2, 4)                   # layers 2, 4, 6 have neural memory, can add more\n",
    "NEURAL_MEM_GATE_ATTN_OUTPUT = False\n",
    "NEURAL_MEM_MOMENTUM = True\n",
    "NEURAL_MEM_MOMENTUM_ORDER = 1\n",
    "NEURAL_MEM_QK_NORM = True\n",
    "NEURAL_MEM_MAX_LR = 1e-1\n",
    "USE_MEM_ATTENTION_MODEL = False\n",
    "WINDOW_SIZE = 32\n",
    "NEURAL_MEM_SEGMENT_LEN = 4                      # set smaller for more granularity for learning rate / momentum etc\n",
    "NEURAL_MEM_BATCH_SIZE = 128                     # set smaller to update the neural memory weights more often as it traverses the sequence\n",
    "SLIDING_WINDOWS = True\n",
    "STORE_ATTN_POOL_CHUNKS = True                   # whether to use attention pooling for chunk derived momentum, per-layer lr mod, decay\n",
    "MEMORY_MODEL_PER_LAYER_LEARNED_LR = True\n",
    "NEURAL_MEM_WEIGHT_RESIDUAL = True               # learning to accept contributions from the weights of the previous neural mem layer brings about significant improvements. this was improvised and not in the paper, but inspired by the value residual learning free lunch paper\n",
    "NEURAL_MEM_QKV_RECEIVES_DIFF_VIEW = True        # will allow the neural memory to select what layers from which to derive queries / keys / values, effectively allowing it to graft itself to the transformer in any way to be beneficial. this is to address an issue from a phd student who noted that the mem network is learning nothing more than wk @ wv. this also generalizes all possible ways to connect the neural memory to a transformer, a sort of NAS\n",
    "NEURAL_MEM_SPEC_NORM_SURPRISES = True           # applying lessons from Muon optimizer to surprise updates, by spectral norming the surprises\n",
    "\n",
    "# experiment related\n",
    "\n",
    "PROJECT_NAME = 'titans-mac-transformer'\n",
    "RUN_NAME = f'mac - {NUM_LONGTERM_MEM} longterm mems, layers {NEURAL_MEM_LAYERS}'\n",
    "WANDB_ONLINE = True # turn this on to pipe experiment to cloud\n",
    "\n",
    "# perf related\n",
    "\n",
    "USE_ACCELERATED_SCAN = torch.cuda.is_available()  # Only works on CUDA\n",
    "USE_FLEX_ATTN = torch.cuda.is_available()  # FlexAttention requires CUDA\n",
    "USE_FAST_INFERENCE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32c451d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mincognitojam\u001b[0m (\u001b[33mincognitojam-dash-software-ltd\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cameron/Developer/titans-pytorch/wandb/run-20251210_130430-pqo3c1kd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/incognitojam-dash-software-ltd/titans-mac-transformer/runs/pqo3c1kd' target=\"_blank\">mac - 4 longterm mems, layers (2, 4)</a></strong> to <a href='https://wandb.ai/incognitojam-dash-software-ltd/titans-mac-transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/incognitojam-dash-software-ltd/titans-mac-transformer' target=\"_blank\">https://wandb.ai/incognitojam-dash-software-ltd/titans-mac-transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/incognitojam-dash-software-ltd/titans-mac-transformer/runs/pqo3c1kd' target=\"_blank\">https://wandb.ai/incognitojam-dash-software-ltd/titans-mac-transformer/runs/pqo3c1kd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/incognitojam-dash-software-ltd/titans-mac-transformer/runs/pqo3c1kd?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x11f43be00>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wandb experiment tracker\n",
    "\n",
    "import wandb\n",
    "wandb.init(project = PROJECT_NAME, name = RUN_NAME, mode = 'disabled' if not WANDB_ONLINE else 'online')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "972128b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "\n",
    "def cycle(loader):\n",
    "    while True:\n",
    "        for data in loader:\n",
    "            yield data\n",
    "\n",
    "def decode_token(token):\n",
    "    return str(chr(max(32, token)))\n",
    "\n",
    "def decode_tokens(tokens):\n",
    "    return ''.join(list(map(decode_token, tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "283de1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory model\n",
    "\n",
    "if USE_MEM_ATTENTION_MODEL:\n",
    "    neural_memory_model = MemoryAttention(\n",
    "        dim = 64\n",
    "    )\n",
    "else:\n",
    "    neural_memory_model = MemoryMLP(\n",
    "        dim = 64,\n",
    "        depth = NEURAL_MEMORY_DEPTH\n",
    "    )\n",
    "\n",
    "# instantiate memory-as-context transformer\n",
    "\n",
    "model = MemoryAsContextTransformer(\n",
    "    num_tokens = 256,\n",
    "    dim = 384,\n",
    "    depth = 8,\n",
    "    segment_len = WINDOW_SIZE,\n",
    "    num_persist_mem_tokens = NUM_PERSIST_MEM,\n",
    "    num_longterm_mem_tokens = NUM_LONGTERM_MEM,\n",
    "    neural_memory_layers = NEURAL_MEM_LAYERS,\n",
    "    neural_memory_segment_len = NEURAL_MEM_SEGMENT_LEN,\n",
    "    neural_memory_batch_size = NEURAL_MEM_BATCH_SIZE,\n",
    "    neural_mem_gate_attn_output = NEURAL_MEM_GATE_ATTN_OUTPUT,\n",
    "    neural_mem_weight_residual = NEURAL_MEM_WEIGHT_RESIDUAL,\n",
    "    neural_memory_qkv_receives_diff_views = NEURAL_MEM_QKV_RECEIVES_DIFF_VIEW,\n",
    "    use_flex_attn = USE_FLEX_ATTN,\n",
    "    sliding_window_attn = SLIDING_WINDOWS,\n",
    "    neural_memory_model = neural_memory_model,\n",
    "    neural_memory_kwargs = dict(\n",
    "        dim_head = 64,\n",
    "        heads = 4,\n",
    "        attn_pool_chunks = STORE_ATTN_POOL_CHUNKS,\n",
    "        qk_rmsnorm = NEURAL_MEM_QK_NORM,\n",
    "        momentum = NEURAL_MEM_MOMENTUM,\n",
    "        momentum_order = NEURAL_MEM_MOMENTUM_ORDER,\n",
    "        default_step_transform_max_lr = NEURAL_MEM_MAX_LR,\n",
    "        use_accelerated_scan = USE_ACCELERATED_SCAN,\n",
    "        per_parameter_lr_modulation = MEMORY_MODEL_PER_LAYER_LEARNED_LR,\n",
    "        spectral_norm_surprises = NEURAL_MEM_SPEC_NORM_SURPRISES\n",
    "    )\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4472fb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare enwik8 data\n",
    "\n",
    "with gzip.open('./data/enwik8.gz') as file:\n",
    "    data = np.frombuffer(file.read(int(95e6)), dtype = np.uint8).copy()\n",
    "    data_train, data_val = np.split(data, [int(90e6)])\n",
    "    data_train, data_val = map(torch.from_numpy, (data_train, data_val))\n",
    "\n",
    "class TextSamplerDataset(Dataset):\n",
    "    def __init__(self, data, seq_len, device):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "        self.device = device\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        rand_start = torch.randint(0, self.data.size(0) - self.seq_len, (1,))\n",
    "        full_seq = self.data[rand_start: rand_start + self.seq_len + 1].long()\n",
    "        return full_seq.to(self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size(0) // self.seq_len\n",
    "\n",
    "train_dataset = TextSamplerDataset(data_train, SEQ_LEN, device)\n",
    "val_dataset   = TextSamplerDataset(data_val, SEQ_LEN, device)\n",
    "train_loader  = cycle(DataLoader(train_dataset, batch_size = BATCH_SIZE))\n",
    "val_loader    = cycle(DataLoader(val_dataset, batch_size = BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a89610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "\n",
    "optim = AdoptAtan2(model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0579757",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|          | 0/2500 [00:00<?, ?it/s]/Users/cameron/Developer/titans-pytorch/.venv/lib/python3.13/site-packages/torch/nn/functional.py:5294: UserWarning: MPS: The constant padding of more than 3 dimensions is not currently supported natively. It uses View Ops default implementation to run. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Pad.mm:468.)\n",
      "  return torch._C._nn.pad(input, pad, mode, value)\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "\n",
    "for i in tqdm.tqdm(range(NUM_BATCHES), mininterval = 10., desc = 'training'):\n",
    "    model.train()\n",
    "\n",
    "    for __ in range(GRADIENT_ACCUMULATE_EVERY):\n",
    "        loss = model(next(train_loader), return_loss = True)\n",
    "        loss.backward()\n",
    "\n",
    "    print(f'training loss: {loss.item()}')\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "    wandb.log(dict(loss = loss.item()))\n",
    "\n",
    "    if i % VALIDATE_EVERY == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            loss = model(next(val_loader), return_loss = True)\n",
    "            print(f'validation loss: {loss.item()}')\n",
    "\n",
    "    if SHOULD_GENERATE and (i % GENERATE_EVERY == 0 or i == NUM_BATCHES - 1):\n",
    "        model.eval()\n",
    "        inp = random.choice(val_dataset)[:PRIME_LENGTH]\n",
    "        prime = decode_tokens(inp)\n",
    "        print(f'%s \\n\\n %s', (prime, '*' * 100))\n",
    "\n",
    "        sample = model.sample(inp[None, ...], GENERATE_LENGTH, use_cache = USE_FAST_INFERENCE)\n",
    "        output_str = decode_tokens(sample[0])\n",
    "        print(output_str)\n",
    "\n",
    "        # Save checkpoint after generation\n",
    "        checkpoint_path = f'checkpoints/titans_mac_step_{i}.pt'\n",
    "        import os\n",
    "        os.makedirs('checkpoints', exist_ok=True)\n",
    "        torch.save({\n",
    "            'step': i,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optim.state_dict(),\n",
    "            'loss': loss.item(),\n",
    "        }, checkpoint_path)\n",
    "        print(f'\\nCheckpoint saved to {checkpoint_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8582fe69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "titans-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
